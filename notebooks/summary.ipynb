{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc498cec",
   "metadata": {},
   "source": [
    "# Multi-Number Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bf7f08",
   "metadata": {},
   "source": [
    "### Approaches for getting more information out of your models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f002c5aa",
   "metadata": {},
   "source": [
    "## Full Bayesian vs. Quantile Regression:  Linear Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f8f4da",
   "metadata": {
    "cell_marker": "'''"
   },
   "source": [
    "When we need our models to produce a full posterior distribution for our \n",
    "predictions, we can bring to bear the full suite of tools from [Bayesian \n",
    "analysis](http://www.stat.columbia.edu/~gelman/book/).  Sometimes, though, we\n",
    "need shortcuts:  using the full Bayesian model may exceed our computational\n",
    "limitations, or we may not have the tooling and infrastructure to readily \n",
    "productionize it.  In such cases, [quantile regression](\n",
    "https://en.wikipedia.org/wiki/Quantile_regression) can be a convenient \n",
    "alternative.\n",
    "\n",
    "Quantile regression lets us predict a chosen [quantile](\n",
    "https://en.wikipedia.org/wiki/Quantiles) (e.g., the median, the 80th percentile)\n",
    "of our predicted outcome variable's distribution.  This distribution isn't \n",
    "exactly the same as the predicted posterior distribution of a full Bayesian\n",
    "model, because it doesn't account for a prior.  However, in cases where the\n",
    "prior is very weak or the large number of cases in our data set lets the \n",
    "likelihood overwhelm the prior's effects on the posterior, the predicted \n",
    "quantiles can be reasonably close to the corresponding positions of the \n",
    "posterior.\n",
    "\n",
    "To illustrate this similarity, I generated some positively correlated bivariate \n",
    "data with Gaussian noise in the response variable, so that I could run some\n",
    "simple regression models on it.  Below is a scatterplot of a sample of the data\n",
    "along with every decile (from the 10th to the 90th) of the predicted posterior \n",
    "distribution of a full Bayesian linear model with weak priors on its parameters.\n",
    "\n",
    "![image](./output/s03_bayes_stan_data03/quantile_plot_x1.png)\n",
    "\n",
    "Next is a scatterplot of the same data along with every decile from quantile\n",
    "regression models, where each model predicts a single decile.\n",
    "\n",
    "![image](./output/s04_quantile_data03/quantile_plot_x1.png)\n",
    "\n",
    "The two plots appear to be very similar.  We can juxtapose the Bayesian and\n",
    "non-Bayesian predictions on a single plot, so that we can compare them more\n",
    "carefully.\n",
    "\n",
    "![image](./output/s10_results/s03_s04_quantiles_data03.png)\n",
    "\n",
    "The two sets of predictions aren't identical, but they're quite close.  We can\n",
    "also look at slices along the x-axis, so that we can see the distributions and\n",
    "the two sets of predictions.  For this purpose, we group the data into 100 bins\n",
    "and show just a few bins along the x-axis.\n",
    "\n",
    "![image](./output/s10_results/s03_s04_density_by_bin_data03.png)\n",
    "\n",
    "The density plots show the Gaussian-distributed noise along the y-axis for each\n",
    "bin.  Accordingly, we can now see the distribution and its predicted deciles\n",
    "together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd4ab4",
   "metadata": {},
   "source": [
    "## Full Bayesian vs. Quantile Regression:  Curvy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a863b32b",
   "metadata": {
    "cell_marker": "'''"
   },
   "source": [
    "The data for our examples above was generated from a linear model, so the \n",
    "Bayesian linear regression and quantile linear regression models readily fit to \n",
    "the data very well.  Our data is often more complicated, so our first attempt \n",
    "at fitting a linear model might not work well.  Here's an example of \n",
    "Bayesian linear regression predictions on a curvier data set.\n",
    "\n",
    "![image](./output/s03_bayes_stan_data02/quantile_plot_x1.png)\n",
    "\n",
    "And here's the quantile regression predictions for the same data.\n",
    "\n",
    "![image](./output/s04_quantile_data02/quantile_plot_x1.png)\n",
    "\n",
    "And here they are superimposed.\n",
    "\n",
    "![image](./output/s10_results/s03_s04_quantiles_data02.png)\n",
    "\n",
    "Clearly, the linear models are having difficulties.  Noticeably, the quantile\n",
    "regression models choose slopes so that the predictions of the deciles \n",
    "separate, or fan out, while one looks from low x-values (left) to high x-values\n",
    "(right).  That divergence might mean that each adjacent pair of deciles \n",
    "actually does bound ~10% of the data points overall, but the decile predictions\n",
    "are clearly poor conditional on *x*.  We can see this even more clearly below\n",
    "where the Gaussian distributions and the decile predictions markedly diverge\n",
    "for several bins.\n",
    "\n",
    "![image](./output/s10_results/s03_s04_density_by_bin_data02.png)\n",
    "\n",
    "Of course, this isn't surprising.  If we really want our linear models to fit\n",
    "non-linear data, we should make them more flexible by, for example, adding\n",
    "[polynomial terms](https://en.wikipedia.org/wiki/Polynomial_regression) or \n",
    "(when we have >1 predictor) [interaction](\n",
    "https://stattrek.com/multiple-regression/interaction) [terms](\n",
    "https://quantifyinghealth.com/why-and-when-to-include-interactions-in-a-regression-model/) \n",
    "or the like.\n",
    "\n",
    "An alternative is to use more flexible modeling algorithms like tree ensembles \n",
    "or neural networks.  The Python package [scikit-learn](\n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html)\n",
    "has some convenient methods for doing this, but I chose to create my own\n",
    "implementation in [PyTorch](https://pytorch.org/).\n",
    "\n",
    "While we can continue to create one model per predicted quantile, it might be\n",
    "more efficient for both training and inference to incorporate all the predicted\n",
    "deciles into a single model.  We can do this with [multi-task learning](\n",
    "https://arxiv.org/abs/1706.05098).  \n",
    "\n",
    "Here are the results from one training run of the model:\n",
    "\n",
    "![image](./output/s06_multitask_nn_data02_1/model_plot_x1.png)\n",
    "\n",
    "Clearly, the more flexible neural network model is adhering more closely to the\n",
    "deciles condtional on *x*.  We can also see this when we compare the results to\n",
    "the Bayesian linear regression decile predictions.\n",
    "\n",
    "![image](./output/s10_results/s03_s06_quantiles_data02.png)\n",
    "\n",
    "![image](./output/s10_results/s03_s06_density_by_bin_data02.png)\n",
    "\n",
    "While the neural network results aren't perfect, they clearly track the Gaussian\n",
    "distributions in the x-bins more closely than the linear results do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222e56b",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d693296",
   "metadata": {
    "cell_marker": "'''"
   },
   "source": [
    "In summary, we sometimes want more information from our model than just a single\n",
    "point estimate of the response variable's mean or median; we might want to know\n",
    "more about the broader range of the prediction.  Bayesian models with \n",
    "informative priors are often our best solution, because they can provide us with \n",
    "a full posterior predictive distribution.  But they do have their drawbacks, \n",
    "including computational intensity.  So a \"compromise\" solution might be quantile\n",
    "regression, which we implemented for a linear model and for a more flexible\n",
    "neural network."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
