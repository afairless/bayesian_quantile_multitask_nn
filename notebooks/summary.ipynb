{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "621ea623",
   "metadata": {},
   "source": [
    "# Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89621e6",
   "metadata": {},
   "source": [
    "## I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f675c",
   "metadata": {
    "cell_marker": "'''"
   },
   "source": [
    "Oftentimes, when we're making predictions from a model, a single number -- a \n",
    "point estimate -- isn't enough.  For example, if we predict that the price of a \n",
    "house is $407,833.00, are we really absolutely sure that that is exactly what\n",
    "the house will sell for, down to the penny?  Well, no, of course not.  Maybe\n",
    "it'll sell for $410,000.  Or maybe just for $399,000.  That doesn't mean our \n",
    "model was \"wrong\", exactly, because both of those prices are fairly close to\n",
    "our prediction.  Really, it means that that one-number prediction was rather\n",
    "bare-bones; it didn't give us a lot of information.  We'd often like to have\n",
    "much more information from our predictions, and we can get that with a Bayesian \n",
    "model that offers a full probability distribution as a prediction.  For our \n",
    "house price example above, it might say that there's a 5% chance that the house\n",
    "will sell at $407,833, a 2% chance that it'll sell at $407,000, a 1% chance that\n",
    "it'll sell at $406,000, and so on until all our probabilities add up to 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b676cb",
   "metadata": {},
   "source": [
    "## Full Bayesian vs. Quantile Regression:  Linear Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad1f508",
   "metadata": {
    "cell_marker": "'''"
   },
   "source": [
    "When we need our models to produce a full posterior distribution for our \n",
    "predictions, we can bring to bear the full suite of tools from [Bayesian \n",
    "analysis](http://www.stat.columbia.edu/~gelman/book/).  Sometimes, though, we\n",
    "need shortcuts:  using the full Bayesian model may exceed our computational\n",
    "limitations, or we may not have the tooling and infrastructure to readily \n",
    "productionize the model.  In such cases, [quantile regression](quantile \n",
    "regression) can be a convenient alternative.\n",
    "\n",
    "Quantile regression lets us predict one or a multitude of chosen [quantiles](\n",
    "https://en.wikipedia.org/wiki/Quantiles) (e.g., the median, the 80th percentile)\n",
    "of our predicted outcome variable's distribution.  This distribution isn't \n",
    "exactly the same as the predicted posterior distribution of a full Bayesian\n",
    "model, because it doesn't account for a prior.  However, in cases where the\n",
    "prior is very weak or the large number of cases in our data set lets the \n",
    "likelihood overwhelm the prior's effects on the posterior, the predicted \n",
    "quantiles can be reasonably close to the corresponding positions of the \n",
    "posterior.\n",
    "\n",
    "To illustrate this similarity, I generated some positively correlated bivariate \n",
    "data with Gaussian noise in the response variable, so that I could run some\n",
    "simple regression models on it.  Below is a scatterplot of a sample of the data\n",
    "along with every decile (from the 10th to the 90th) of the predicted posterior \n",
    "distribution of a full Bayesian linear model with weak priors on its parameters.\n",
    "\n",
    "![image](./output/s03_bayes_stan_data03/quantile_plot_x1.png)\n",
    "\n",
    "Next is a scatterplot of the same data along with every decile from quantile\n",
    "regression models, where each model predicts a single decile.\n",
    "\n",
    "![image](./output/s04_quantile_data03/quantile_plot_x1.png)\n",
    "\n",
    "The two plots appear to be very similar.  We can juxtapose the Bayesian and\n",
    "non-Bayesian predictions on a single plot, so that we can compare them more\n",
    "carefully.\n",
    "\n",
    "![image](./output/s10_results/s03_s04_quantiles_data03.png)\n",
    "\n",
    "The two sets of predictions aren't identical, but they're quite close.  We can\n",
    "also look at slices along the x-axis, so that we can see the distributions and\n",
    "the two sets of predictions.  For this purpose, we group the data into 100 bins\n",
    "and show just a few bins along the x-axis.\n",
    "\n",
    "![image](./output/s10_results/s03_s04_density_by_bin_data03.png)\n",
    "\n",
    "The density plots show the Gaussian-distributed noise along the y-axis for each\n",
    "bin.  Accordingly, we can now see the distribution and its predicted deciles\n",
    "together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992abec",
   "metadata": {},
   "source": [
    "## Full Bayesian vs. Quantile Regression:  Curvy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f9bb0",
   "metadata": {
    "cell_marker": "'''"
   },
   "source": [
    "The data for our examples above was generated from a linear model, so the \n",
    "Bayesian linear regression and quantile linear regression models readily fit to \n",
    "the data very well.  Our data is often more complicated, so our first attempt \n",
    "at fitting a linear model might not work well.  Here's an example of \n",
    "Bayesian linear regression predictions on a curvier data set.\n",
    "\n",
    "![image](./output/s03_bayes_stan_data02/quantile_plot_x1.png)\n",
    "\n",
    "And here's the quantile regression predictions for the same data.\n",
    "\n",
    "![image](./output/s04_quantile_data03/quantile_plot_x1.png)\n",
    "\n",
    "And here they are superimposed.\n",
    "\n",
    "![image](./output/s10_results/s03_s04_quantiles_data02.png)\n",
    "\n",
    "Clearly, the linear models are having difficulties.  Noticeably, the quantile\n",
    "regression models choose slopes so that the predictions of the deciles \n",
    "separate, or fan out, while one looks from low x-values (left) to high x-values\n",
    "(right).  That divergence might mean that each adjacent pair of deciles \n",
    "actually does bound ~10% of the data points overall, but the decile predictions\n",
    "are clearly poor conditional on *x*.  We can see this even more clearly below\n",
    "where the Gaussian distributions and the decile predictions markedly diverge\n",
    "for several bins.\n",
    "\n",
    "![image](./output/s10_results/s03_s04_density_by_bin_data02.png)\n",
    "\n",
    "Of course, this isn't surprising.  If we really want our linear models to fit\n",
    "non-linear data, we should make them more flexible by, for example, adding\n",
    "polynomial terms or (when we have >1 predictor) interaction terms or the like.\n",
    "\n",
    "An alternative is to use more flexible modeling algorithms like tree ensembles \n",
    "or neural networks.  The Python package [scikit-learn](\n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html)\n",
    "has some convenient methods for doing this, but I chose to create my own\n",
    "implementation in PyTorch.\n",
    "\n",
    "While we can continue to create one model per predicted quantile, it might be\n",
    "more efficient for both training and inference to incorporate all the predicted\n",
    "deciles into a single model.  We can do this with [multi-task learning](\n",
    "https://arxiv.org/abs/1706.05098).  \n",
    "\n",
    "Here are the results from one training run of the model:\n",
    "\n",
    "![image](./output/s10_results/s03_s04_density_by_bin_data02.png)\n",
    "\n",
    "Clearly, the more flexible neural network model is adhering more closely to the\n",
    "deciles condtional on *x*.  We can also see this when we compare the results to\n",
    "the Bayesian linear regression.\n",
    "\n",
    "![image](./output/s10_results/s03_s06_quantiles_data02.png)\n",
    "\n",
    "![image](./output/s10_results/s03_s06_density_by_bin_data02.png)\n",
    "\n",
    "While the neural network results aren't perfect, they clearly track the Gaussian\n",
    "distributions in the x-bins more closely than the linear results do."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
